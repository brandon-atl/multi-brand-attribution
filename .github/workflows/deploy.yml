name: Deploy to Snowflake

'on':
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare runner (system deps)
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends ca-certificates curl libffi-dev libssl-dev

      # ───────────────────────────────────────────────────────────────────────────
      # Preflight: Patch Python UDFs
      # - Strip Snowpark imports from any UDFs (unsupported in UDF runtime)
      # - Add RUNTIME_VERSION and PACKAGES=('pandas','numpy') if imported in UDF code
      # ───────────────────────────────────────────────────────────────────────────
      - name: Patch Python UDFs (remove Snowpark imports, add PACKAGES)
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Run UDF patcher
        shell: bash
        run: |
          set -euxo pipefail
          python - <<'PY'
          import re, sys, subprocess
          from pathlib import Path

          UDF_RE = re.compile(
              r'(?is)'
              r'(CREATE\s+(?:OR\s+REPLACE\s+)?FUNCTION\b.*?\bLANGUAGE\s+PYTHON\b)'
              r'(?P<opts>.*?)'
              r'(?P<as>AS\s+\$\$)'
              r'(?P<code>.*?)'
              r'(\$\$;)'
          )

          def strip_snowpark_imports(code: str) -> str:
              code = re.sub(
                  r'(?m)^\s*(?:from\s+snowflake\.snowpark\s+import\b.*|import\s+snowflake\.snowpark(?:\s+as\s+\w+)?\b.*)\s*$',
                  '',
                  code
              )
              code = re.sub(r':\s*Session\b', ': object', code)
              return code

          def ensure_opts(opts: str, code: str) -> str:
              new_opts = opts
              if re.search(r'(?i)\bRUNTIME_VERSION\b', new_opts) is None:
                  new_opts += "\n  RUNTIME_VERSION = '3.10'"
              needs_pandas = re.search(r'(?i)\b(from|import)\s+pandas\b', code) is not None
              needs_numpy  = re.search(r'(?i)\b(from|import)\s+numpy\b',  code) is not None
              if needs_pandas or needs_numpy:
                  pkg_m = re.search(r'(?i)\bPACKAGES\s*=\s*\((.*?)\)', new_opts)
                  if pkg_m is None:
                      pkgs = []
                      if needs_pandas: pkgs.append("'pandas'")
                      if needs_numpy:  pkgs.append("'numpy'")
                      new_opts += f"\n  PACKAGES = ({', '.join(pkgs)})"
                  else:
                      pkgs_text = pkg_m.group(1)
                      pkgs_set = {
                          p.strip().strip("'").strip('"') for p in pkgs_text.split(",") if p.strip()
                      }
                      if needs_pandas: pkgs_set.add("pandas")
                      if needs_numpy:  pkgs_set.add("numpy")
                      new_pkgs = ", ".join(sorted(f"'{p}'" for p in pkgs_set))
                      new_opts = re.sub(r'(?i)\bPACKAGES\s*=\s*\((.*?)\)', f"PACKAGES = ({new_pkgs})", new_opts, 1)
              return new_opts

          def patch_file(path: Path) -> bool:
              s = path.read_text(encoding="utf-8", errors="ignore")
              changed = False

              def _patch(m: re.Match) -> str:
                  nonlocal changed
                  head, opts, as_tok, code = m.group(1), m.group("opts"), m.group("as"), m.group("code")
                  code2 = strip_snowpark_imports(code)
                  opts2 = ensure_opts(opts, code2)
                  if code2 != code or opts2 != opts:
                      changed = True
                  return head + opts2 + "\n" + as_tok + code2 + "$$;"

              new_s = re.sub(UDF_RE, _patch, s)
              if changed and new_s != s:
                  path.write_text(new_s, encoding="utf-8")
                  print(f"Patched UDFs in: {path}")
                  return True
              return False

          def collect_sql_files():
              try:
                  out = subprocess.check_output(["git", "ls-files", "*.sql"], text=True).strip().splitlines()
                  return [Path(p) for p in out]
              except Exception:
                  return list(Path(".").rglob("*.sql"))

          modified = [str(p) for p in collect_sql_files() if patch_file(p)]
          print("Patched files:" if modified else "No UDF patches needed.", modified)
          PY

      # ───────────────────────────────────────────────────────────────────────────
      # Install SnowSQL (proper major.minor bootstrap path)
      # ───────────────────────────────────────────────────────────────────────────
      - name: Install SnowSQL (with logs)
        id: snowsql
        continue-on-error: true
        env:
          SNOWSQL_VERSION: "1.2.31"
        run: |
          set -euxo pipefail
          curl -fsSL \
            https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-${SNOWSQL_VERSION}-linux_x86_64.bash \
            -o snowsql.bash
          bash snowsql.bash -y > snowsql_install.log 2>&1 || true
          SNOWSQL_BIN="$HOME/.snowsql/snowsql"
          if [ -x "$SNOWSQL_BIN" ]; then
            echo "ok=true" >> "$GITHUB_OUTPUT"
            "$SNOWSQL_BIN" --version || true
          else
            echo "ok=false" >> "$GITHUB_OUTPUT"
            echo "==== SnowSQL install log (tail) ===="
            tail -200 snowsql_install.log || true
          fi

      - name: Show decision
        run: echo "SnowSQL available? -> ${{ steps.snowsql.outputs.ok }}"

      # ───────────────────────────────────────────────────────────────────────────
      # Bootstrap (tables only): matches the four tables in your Snowpark worksheet
      # ───────────────────────────────────────────────────────────────────────────
      - name: Bootstrap via SnowSQL
        if: steps.snowsql.outputs.ok == 'true'
        env:
          SNOWSQL_ACCOUNT:   ${{ secrets.SNOW_ACCOUNT }}
          SNOWSQL_USER:      ${{ secrets.SNOW_USER }}
          SNOWSQL_PWD:       ${{ secrets.SNOW_PASSWORD }}
          SNOWSQL_ROLE:      ${{ secrets.SNOW_ROLE }}
          SNOWSQL_WAREHOUSE: ${{ secrets.SNOW_WAREHOUSE }}
          SNOWSQL_DATABASE:  ${{ secrets.SNOW_DATABASE }}
          SNOWSQL_SCHEMA:    ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          cat > bootstrap.sql <<SQL
          USE ROLE ${SNOWSQL_ROLE};
          USE WAREHOUSE ${SNOWSQL_WAREHOUSE};
          USE DATABASE ${SNOWSQL_DATABASE};
          CREATE SCHEMA IF NOT EXISTS ${SNOWSQL_SCHEMA};
          USE SCHEMA ${SNOWSQL_SCHEMA};

          CREATE OR REPLACE TABLE DIM_BRAND (
            BRAND_ID INT,
            BRAND_CODE STRING
          );

          CREATE OR REPLACE TABLE DIM_CHANNEL (
            CHANNEL_ID INT,
            CHANNEL_NAME STRING
          );

          CREATE OR REPLACE TABLE DIM_CUSTOMER (
            CUSTOMER_ID INT,
            SIGNUP_UTC TIMESTAMP_NTZ,
            COUNTRY STRING
          );

          CREATE OR REPLACE TABLE FACT_ORDERS (
            ORDER_ID INT,
            CUSTOMER_ID INT,
            ORDER_UTC TIMESTAMP_NTZ,
            BRAND STRING,
            CHANNEL STRING,
            UNITS NUMBER(9,0),
            REVENUE NUMBER(12,2),
            MARGIN NUMBER(12,2)
          );
          SQL

          "$HOME/.snowsql/snowsql" \
            -a "$SNOWSQL_ACCOUNT" -u "$SNOWSQL_USER" -r "$SNOWSQL_ROLE" \
            -w "$SNOWSQL_WAREHOUSE" -d "$SNOWSQL_DATABASE" -f bootstrap.sql

      # ───────────────────────────────────────────────────────────────────────────
      # Deploy repo SQL (after patching) — if you keep SQL files in repo
      # ───────────────────────────────────────────────────────────────────────────
      - name: Deploy repo SQL (patched UDFs)
        if: steps.snowsql.outputs.ok == 'true'
        env:
          SNOWSQL_ACCOUNT:   ${{ secrets.SNOW_ACCOUNT }}
          SNOWSQL_USER:      ${{ secrets.SNOW_USER }}
          SNOWSQL_PWD:       ${{ secrets.SNOW_PASSWORD }}
          SNOWSQL_ROLE:      ${{ secrets.SNOW_ROLE }}
          SNOWSQL_WAREHOUSE: ${{ secrets.SNOW_WAREHOUSE }}
          SNOWSQL_DATABASE:  ${{ secrets.SNOW_DATABASE }}
          SNOWSQL_SCHEMA:    ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          while IFS= read -r f; do
            echo ">>> Running $f"
            "$HOME/.snowsql/snowsql" \
              -a "$SNOWSQL_ACCOUNT" -u "$SNOWSQL_USER" -r "$SNOWSQL_ROLE" \
              -w "$SNOWSQL_WAREHOUSE" -d "$SNOWSQL_DATABASE" -f "$f"
          done < <(git ls-files '*.sql' | sort)

      # ───────────────────────────────────────────────────────────────────────────
      # Seed data via Snowpark STORED PROCEDURE (pandas/numpy OK), then create views
      # ───────────────────────────────────────────────────────────────────────────
      - name: Seed + Views (Snowpark SP + views)
        if: steps.snowsql.outputs.ok == 'true'
        env:
          SNOWSQL_ACCOUNT:   ${{ secrets.SNOW_ACCOUNT }}
          SNOWSQL_USER:      ${{ secrets.SNOW_USER }}
          SNOWSQL_PWD:       ${{ secrets.SNOW_PASSWORD }}
          SNOWSQL_ROLE:      ${{ secrets.SNOW_ROLE }}
          SNOWSQL_WAREHOUSE: ${{ secrets.SNOW_WAREHOUSE }}
          SNOWSQL_DATABASE:  ${{ secrets.SNOW_DATABASE }}
          SNOWSQL_SCHEMA:    ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          cat > seed_and_views.sql <<'SQL'
          USE ROLE ${SNOWSQL_ROLE};
          USE WAREHOUSE ${SNOWSQL_WAREHOUSE};
          USE DATABASE ${SNOWSQL_DATABASE};
          USE SCHEMA ${SNOWSQL_SCHEMA};

          -- Snowpark-enabled STORED PROCEDURE mirrors your Snowpark worksheet logic
          CREATE OR REPLACE PROCEDURE GENERATE_DEMO_DATA()
          RETURNS STRING
          LANGUAGE PYTHON
          RUNTIME_VERSION = '3.10'
          PACKAGES = ('snowflake-snowpark-python','pandas','numpy')
          HANDLER = 'run'
          AS
          $$
          from snowflake.snowpark import Session
          import pandas as pd, numpy as np

          def run(session: Session) -> str:
              rng = np.random.default_rng(42)
              BRANDS   = ["JCREW","FACTORY","MADEWELL"]
              CHANNELS = ["Direct","Email","Paid Search","Social","Affiliate","Display"]
              N = 50_000
              START = pd.Timestamp("2024-09-01 00:00:00")
              END   = pd.Timestamp("2025-08-31 23:59:59")
              SPAN  = (END - START).total_seconds()

              session.create_dataframe(
                  pd.DataFrame({"BRAND_ID":[1,2,3],"BRAND_CODE":BRANDS})
              ).write.mode("overwrite").save_as_table("DIM_BRAND")

              session.create_dataframe(
                  pd.DataFrame({"CHANNEL_ID":list(range(1,len(CHANNELS)+1)),"CHANNEL_NAME":CHANNELS})
              ).write.mode("overwrite").save_as_table("DIM_CHANNEL")

              signups = START + pd.to_timedelta(np.random.default_rng(7).uniform(0, SPAN, size=N), unit="s")
              cust = pd.DataFrame({
                  "CUSTOMER_ID": np.arange(1, N + 1, dtype="int64"),
                  "SIGNUP_UTC":  pd.to_datetime(signups),
                  "COUNTRY":     rng.choice(["US","CA","UK"], size=N, p=[.84,.10,.06])
              })
              session.write_pandas(cust, "DIM_CUSTOMER", auto_create_table=True, overwrite=True)

              rows = []
              oid = 1
              k_per = np.clip(rng.poisson(1.4, size=N), 0, 6).astype(int)

              for cid, k in zip(range(1, N + 1), k_per):
                  if k == 0: 
                      continue
                  times = np.sort(START + pd.to_timedelta(rng.uniform(0, SPAN, size=k), unit="s"))
                  bsel  = rng.choice(BRANDS,   size=k, p=[.55,.30,.15])
                  chsel = rng.choice(CHANNELS, size=k, p=[.45,.12,.18,.15,.06,.04])

                  for t, b, c in zip(times, bsel, chsel):
                      base_price = 120 if b == "JCREW" else 85 if b == "MADEWELL" else 70
                      units = max(1, int(np.round(rng.gamma(2.0, 0.8))))
                      price = max(25.0, float(rng.normal(base_price, 18)))
                      rev   = round(units * price, 2)
                      margin= round(rev * float(rng.uniform(0.48, 0.63)), 2)

                      rows.append((
                          oid, cid, pd.to_datetime(t), b, c, int(units), rev, margin
                      ))
                      oid += 1

              orders = pd.DataFrame(rows, columns=[
                  "ORDER_ID", "CUSTOMER_ID", "ORDER_UTC", "BRAND", "CHANNEL", "UNITS", "REVENUE", "MARGIN"
              ])
              orders["ORDER_ID"]    = orders["ORDER_ID"].astype("int64")
              orders["CUSTOMER_ID"] = orders["CUSTOMER_ID"].astype("int64")
              orders["UNITS"]       = orders["UNITS"].astype("int64")
              orders["ORDER_UTC"]   = pd.to_datetime(orders["ORDER_UTC"])

              session.write_pandas(orders, "FACT_ORDERS", auto_create_table=True, overwrite=True)

              total_orders = int(session.sql("SELECT COUNT(*) FROM FACT_ORDERS").collect()[0][0])
              return f"Generated {total_orders:,} orders across {N:,} customers into FACT_ORDERS. Dimensions and views created."
          $$;

          -- Views: identical logic to your worksheet
          CREATE OR REPLACE VIEW VW_FIRST_PURCHASE AS
          WITH first_ts AS (
            SELECT CUSTOMER_ID, MIN(ORDER_UTC) AS FIRST_ORDER_UTC
            FROM FACT_ORDERS
            GROUP BY 1
          )
          SELECT
            o.CUSTOMER_ID,
            o.BRAND AS FIRST_PURCHASE_BRAND,
            f.FIRST_ORDER_UTC
          FROM FACT_ORDERS o
          JOIN first_ts f USING (CUSTOMER_ID)
          WHERE o.ORDER_UTC = f.FIRST_ORDER_UTC
          QUALIFY ROW_NUMBER() OVER (
            PARTITION BY o.CUSTOMER_ID ORDER BY o.ORDER_UTC, o.ORDER_ID
          ) = 1;

          CREATE OR REPLACE VIEW VW_BRAND_FLOWS_ANY AS
          SELECT
            fp.FIRST_PURCHASE_BRAND AS FROM_BRAND,
            o.BRAND                 AS TO_BRAND,
            COUNT(DISTINCT o.CUSTOMER_ID) AS CUSTOMERS
          FROM FACT_ORDERS o
          JOIN VW_FIRST_PURCHASE fp USING (CUSTOMER_ID)
          GROUP BY 1,2;

          CREATE OR REPLACE VIEW VW_BRAND_FLOWS_SUBSEQ AS
          WITH first_ts AS (
            SELECT CUSTOMER_ID, MIN(ORDER_UTC) AS FIRST_ORDER_UTC
            FROM FACT_ORDERS
            GROUP BY 1
          )
          SELECT
            fp.FIRST_PURCHASE_BRAND AS FROM_BRAND,
            o.BRAND                 AS TO_BRAND,
            COUNT(DISTINCT o.CUSTOMER_ID) AS CUSTOMERS
          FROM FACT_ORDERS o
          JOIN first_ts ft USING (CUSTOMER_ID)
          JOIN VW_FIRST_PURCHASE fp USING (CUSTOMER_ID)
          WHERE o.ORDER_UTC > ft.FIRST_ORDER_UTC
          GROUP BY 1,2;

          -- Populate data
          CALL GENERATE_DEMO_DATA();
          SQL

          sed -i "s/\${SNOWSQL_ROLE}/${SNOWSQL_ROLE}/g"           seed_and_views.sql
          sed -i "s/\${SNOWSQL_WAREHOUSE}/${SNOWSQL_WAREHOUSE}/g" seed_and_views.sql
          sed -i "s/\${SNOWSQL_DATABASE}/${SNOWSQL_DATABASE}/g"   seed_and_views.sql
          sed -i "s/\${SNOWSQL_SCHEMA}/${SNOWSQL_SCHEMA}/g"       seed_and_views.sql

          "$HOME/.snowsql/snowsql" \
            -a "$SNOWSQL_ACCOUNT" -u "$SNOWSQL_USER" -r "$SNOWSQL_ROLE" \
            -w "$SNOWSQL_WAREHOUSE" -d "$SNOWSQL_DATABASE" \
            -f seed_and_views.sql

      # ───────────────────────────────────────────────────────────────────────────
      # Python fallback (if SnowSQL unavailable): also creates SP, calls it, then views
      # ───────────────────────────────────────────────────────────────────────────
      - name: Python fallback — install connector
        if: steps.snowsql.outputs.ok != 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Python fallback — run deploy script
        if: steps.snowsql.outputs.ok != 'true'
        env:
          SNOW_HOST:            ${{ secrets.SNOW_HOST }}
          SNOW_ACCOUNT:         ${{ secrets.SNOW_ACCOUNT }}
          SNOWFLAKE_ACCOUNT:    ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOW_USER:            ${{ secrets.SNOW_USER }}
          SNOW_PASSWORD:        ${{ secrets.SNOW_PASSWORD }}
          SNOW_ROLE:            ${{ secrets.SNOW_ROLE }}
          SNOW_WAREHOUSE:       ${{ secrets.SNOW_WAREHOUSE }}
          SNOW_DATABASE:        ${{ secrets.SNOW_DATABASE }}
          SNOW_SCHEMA:          ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip
          python -m pip install snowflake-connector-python pandas numpy

          python - <<'PY'
          import os, snowflake.connector as sf
          acct=(os.getenv('SNOW_ACCOUNT') or os.getenv('SNOWFLAKE_ACCOUNT') or '').strip()
          host=(os.getenv('SNOW_HOST') or '').strip()
          user=os.environ['SNOW_USER']; pwd=os.environ['SNOW_PASSWORD']
          role=os.environ['SNOW_ROLE']; wh=os.environ['SNOW_WAREHOUSE']
          db=os.environ['SNOW_DATABASE']; sc=os.environ['SNOW_SCHEMA']
          kw=dict(user=user,password=pwd,role=role,warehouse=wh,database=db,schema=sc)
          if acct: kw['account']=acct
          if host: kw['host']=host
          if ('account' not in kw) and ('host' not in kw):
              raise RuntimeError("Provide SNOW_ACCOUNT (or SNOWFLAKE_ACCOUNT) or SNOW_HOST.")
          ctx=sf.connect(**kw); cs=ctx.cursor()
          try:
              def exec(sql): print("SQL>", sql.splitlines()[0][:120], "..."); cs.execute(sql)

              exec(f"USE ROLE {role}")
              exec(f"USE WAREHOUSE {wh}")
              exec(f"USE DATABASE {db}")
              exec(f"CREATE SCHEMA IF NOT EXISTS {sc}")
              exec(f"USE SCHEMA {sc}")

              # Four tables (match worksheet)
              exec("""CREATE OR REPLACE TABLE DIM_BRAND (BRAND_ID INT, BRAND_CODE STRING)""")
              exec("""CREATE OR REPLACE TABLE DIM_CHANNEL (CHANNEL_ID INT, CHANNEL_NAME STRING)""")
              exec("""CREATE OR REPLACE TABLE DIM_CUSTOMER (CUSTOMER_ID INT, SIGNUP_UTC TIMESTAMP_NTZ, COUNTRY STRING)""")
              exec("""CREATE OR REPLACE TABLE FACT_ORDERS (
                        ORDER_ID INT, CUSTOMER_ID INT, ORDER_UTC TIMESTAMP_NTZ,
                        BRAND STRING, CHANNEL STRING, UNITS NUMBER(9,0), REVENUE NUMBER(12,2), MARGIN NUMBER(12,2))""")

              # Create Snowpark SP (Snowpark runs inside SP; OK to import)
              sp = r"""
              CREATE OR REPLACE PROCEDURE GENERATE_DEMO_DATA()
              RETURNS STRING
              LANGUAGE PYTHON
              RUNTIME_VERSION='3.10'
              PACKAGES=('snowflake-snowpark-python','pandas','numpy')
              HANDLER='run'
              AS
              $$
              from snowflake.snowpark import Session
              import pandas as pd, numpy as np
              def run(session: Session) -> str:
                  rng = np.random.default_rng(42)
                  BRANDS   = ["JCREW","FACTORY","MADEWELL"]
                  CHANNELS = ["Direct","Email","Paid Search","Social","Affiliate","Display"]
                  N = 50_000
                  START = pd.Timestamp("2024-09-01 00:00:00")
                  END   = pd.Timestamp("2025-08-31 23:59:59")
                  SPAN  = (END - START).total_seconds()

                  session.create_dataframe(pd.DataFrame({"BRAND_ID":[1,2,3],"BRAND_CODE":BRANDS})).write.mode("overwrite").save_as_table("DIM_BRAND")
                  session.create_dataframe(pd.DataFrame({"CHANNEL_ID":list(range(1,len(CHANNELS)+1)),"CHANNEL_NAME":CHANNELS})).write.mode("overwrite").save_as_table("DIM_CHANNEL")

                  signups = START + pd.to_timedelta(np.random.default_rng(7).uniform(0, SPAN, size=N), unit="s")
                  cust = pd.DataFrame({
                      "CUSTOMER_ID": np.arange(1, N + 1, dtype="int64"),
                      "SIGNUP_UTC":  pd.to_datetime(signups),
                      "COUNTRY":     rng.choice(["US","CA","UK"], size=N, p=[.84,.10,.06])
                  })
                  session.write_pandas(cust, "DIM_CUSTOMER", auto_create_table=True, overwrite=True)

                  rows = []; oid = 1
                  k_per = np.clip(rng.poisson(1.4, size=N), 0, 6).astype(int)
                  for cid, k in zip(range(1, N + 1), k_per):
                      if k == 0: continue
                      times = np.sort(START + pd.to_timedelta(rng.uniform(0, SPAN, size=k), unit="s"))
                      bsel  = rng.choice(BRANDS,   size=k, p=[.55,.30,.15])
                      chsel = rng.choice(CHANNELS, size=k, p=[.45,.12,.18,.15,.06,.04])
                      for t, b, c in zip(times, bsel, chsel):
                          base_price = 120 if b == "JCREW" else 85 if b == "MADEWELL" else 70
                          units = max(1, int(np.round(rng.gamma(2.0, 0.8))))
                          price = max(25.0, float(rng.normal(base_price, 18)))
                          rev   = round(units * price, 2)
                          margin= round(rev * float(rng.uniform(0.48, 0.63)), 2)
                          rows.append((oid, cid, pd.to_datetime(t), b, c, int(units), rev, margin))
                          oid += 1

                  orders = pd.DataFrame(rows, columns=[
                      "ORDER_ID","CUSTOMER_ID","ORDER_UTC","BRAND","CHANNEL","UNITS","REVENUE","MARGIN"
                  ])
                  orders["ORDER_ID"]    = orders["ORDER_ID"].astype("int64")
                  orders["CUSTOMER_ID"] = orders["CUSTOMER_ID"].astype("int64")
                  orders["UNITS"]       = orders["UNITS"].astype("int64")
                  orders["ORDER_UTC"]   = pd.to_datetime(orders["ORDER_UTC"])
                  session.write_pandas(orders, "FACT_ORDERS", auto_create_table=True, overwrite=True)

                  total_orders = int(session.sql("SELECT COUNT(*) FROM FACT_ORDERS").collect()[0][0])
                  return f"Generated {total_orders:,} orders across {N:,} customers into FACT_ORDERS."
              $$;
              """
              exec(sp)

              # Views (identical logic)
              exec("""
              CREATE OR REPLACE VIEW VW_FIRST_PURCHASE AS
              WITH first_ts AS (
                SELECT CUSTOMER_ID, MIN(ORDER_UTC) AS FIRST_ORDER_UTC
                FROM FACT_ORDERS
                GROUP BY 1
              )
              SELECT
                o.CUSTOMER_ID,
                o.BRAND AS FIRST_PURCHASE_BRAND,
                f.FIRST_ORDER_UTC
              FROM FACT_ORDERS o
              JOIN first_ts f USING (CUSTOMER_ID)
              WHERE o.ORDER_UTC = f.FIRST_ORDER_UTC
              QUALIFY ROW_NUMBER() OVER (
                PARTITION BY o.CUSTOMER_ID ORDER BY o.ORDER_UTC, o.ORDER_ID
              ) = 1;
              """)

              exec("""
              CREATE OR REPLACE VIEW VW_BRAND_FLOWS_ANY AS
              SELECT
                fp.FIRST_PURCHASE_BRAND AS FROM_BRAND,
                o.BRAND                 AS TO_BRAND,
                COUNT(DISTINCT o.CUSTOMER_ID) AS CUSTOMERS
              FROM FACT_ORDERS o
              JOIN VW_FIRST_PURCHASE fp USING (CUSTOMER_ID)
              GROUP BY 1,2;
              """)

              exec("""
              CREATE OR REPLACE VIEW VW_BRAND_FLOWS_SUBSEQ AS
              WITH first_ts AS (
                SELECT CUSTOMER_ID, MIN(ORDER_UTC) AS FIRST_ORDER_UTC
                FROM FACT_ORDERS
                GROUP BY 1
              )
              SELECT
                fp.FIRST_PURCHASE_BRAND AS FROM_BRAND,
                o.BRAND                 AS TO_BRAND,
                COUNT(DISTINCT o.CUSTOMER_ID) AS CUSTOMERS
              FROM FACT_ORDERS o
              JOIN first_ts ft USING (CUSTOMER_ID)
              JOIN VW_FIRST_PURCHASE fp USING (CUSTOMER_ID)
              WHERE o.ORDER_UTC > ft.FIRST_ORDER_UTC
              GROUP BY 1,2;
              """)

              exec("CALL GENERATE_DEMO_DATA()")
          finally:
              cs.close(); ctx.close()
          PY
