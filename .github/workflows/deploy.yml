name: Deploy to Snowflake

on:
  push: { branches: [ "main" ] }
  workflow_dispatch: {}

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare runner (system deps)
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends ca-certificates curl libffi-dev libssl-dev

      # ───────────────────────────────────────────────────────────────────────────
      # Preflight: Patch Python UDFs to declare PACKAGES for pandas/numpy;
      #            fail fast if any UDF imports snowflake.snowpark
      # ───────────────────────────────────────────────────────────────────────────
      - name: Patch Python UDFs (add PACKAGES, block Snowpark)
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Run UDF patcher
        run: |
          set -euxo pipefail
          python - <<'PY'
          import re, sys
          from pathlib import Path

          # Scan all tracked SQL files for Python UDFs
          sql_files = []
          # Prefer tracked files (honors .gitignore) if available; fall back to glob.
          try:
              import subprocess
              out = subprocess.check_output(["git", "ls-files", "*.sql"], text=True).strip().splitlines()
              sql_files = [Path(p) for p in out]
          except Exception:
              sql_files = list(Path(".").rglob("*.sql"))

          UDF_RE = re.compile(
              r'(?is)'                                   # flags
              r'(CREATE\s+(?:OR\s+REPLACE\s+)?FUNCTION\b.*?\bLANGUAGE\s+PYTHON\b)'  # g1: head through "LANGUAGE PYTHON"
              r'(?P<opts>.*?)'                           # options between LANGUAGE and AS $$
              r'(?P<as>AS\s+\$\$)'                       # g: AS $$
              r'(?P<code>.*?)'                           # code body
              r'(\$\$;)'                                 # end marker
          )

          modified = []
          for path in sql_files:
              s = path.read_text(encoding="utf-8", errors="ignore")
              changed = False

              def patch(m: re.Match) -> str:
                  nonlocal changed
                  opts, code = m.group("opts"), m.group("code")

                  # Guard: Snowpark inside a UDF is not supported; require SPROC instead.
                  if re.search(r'(?i)\bfrom\s+snowflake\.snowpark\b|\bimport\s+snowflake\.snowpark\b', code):
                      print(f"ERROR: {path} defines a Python UDF that imports snowflake.snowpark. "
                            f"Move that logic to a Python stored procedure (Snowpark is supported there).",
                            file=sys.stderr)
                      sys.exit(2)

                  # Detect pandas / numpy usage in the UDF code
                  needs_pandas = re.search(r'(?i)\b(from|import)\s+pandas\b', code) is not None
                  needs_numpy  = re.search(r'(?i)\b(from|import)\s+numpy\b',  code) is not None

                  new_opts = opts

                  # Ensure RUNTIME_VERSION
                  if re.search(r'(?i)\bRUNTIME_VERSION\b', new_opts) is None:
                      new_opts += "\n  RUNTIME_VERSION = '3.10'"

                  # Ensure PACKAGES present if pandas/numpy are imported
                  if needs_pandas or needs_numpy:
                      pkg_line = re.search(r'(?i)\bPACKAGES\s*=\s*\((.*?)\)', new_opts)
                      if pkg_line is None:
                          pkgs = []
                          if needs_pandas: pkgs.append("'pandas'")
                          if needs_numpy:  pkgs.append("'numpy'")
                          new_opts += f"\n  PACKAGES = ({','.join(pkgs)})"
                      else:
                          # Add missing ones into existing PACKAGES
                          pkgs_text = pkg_line.group(1)
                          pkgs_set = {p.strip().strip("'").strip('"') for p in pkgs_text.split(",") if p.strip()}
                          if needs_pandas: pkgs_set.add("pandas")
                          if needs_numpy:  pkgs_set.add("numpy")
                          new_pkgs = ", ".join(sorted(f"'{p}'" for p in pkgs_set))
                          new_opts = re.sub(r'(?i)\bPACKAGES\s*=\s*\((.*?)\)',
                                            f"PACKAGES = ({new_pkgs})", new_opts, count=1)

                  changed = True
                  return m.group(1) + new_opts + "\n" + m.group("as") + code + "$$;"

              new_s = re.sub(UDF_RE, patch, s)
              if changed and new_s != s:
                  path.write_text(new_s, encoding="utf-8")
                  modified.append(str(path))

          print("Patched UDF files:" if modified else "No Python UDFs needed patching.", modified)
          PY

      # ───────────────────────────────────────────────────────────────────────────
      # Install SnowSQL
      # ───────────────────────────────────────────────────────────────────────────
      - name: Install SnowSQL (with logs)
        id: snowsql
        continue-on-error: true
        env:
          SNOWSQL_VERSION: "1.2.31"   # 1.2.x line
        run: |
          set -euxo pipefail
          curl -fsSL \
            https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-${SNOWSQL_VERSION}-linux_x86_64.bash \
            -o snowsql.bash
          bash snowsql.bash -y > snowsql_install.log 2>&1 || true
          SNOWSQL_BIN="$HOME/.snowsql/snowsql"
          if [ -x "$SNOWSQL_BIN" ]; then
            echo "ok=true" >> $GITHUB_OUTPUT
            "$SNOWSQL_BIN" --version || true
          else
            echo "ok=false" >> $GITHUB_OUTPUT
            echo "==== SnowSQL install log (tail) ===="
            tail -200 snowsql_install.log || true
          fi

      - name: Show decision
        run: echo "SnowSQL available? -> ${{ steps.snowsql.outputs.ok }}"

      # ───────────────────────────────────────────────────────────────────────────
      # Bootstrap core objects
      # ───────────────────────────────────────────────────────────────────────────
      - name: Bootstrap via SnowSQL
        if: steps.snowsql.outputs.ok == 'true'
        env:
          SNOWSQL_ACCOUNT:   ${{ secrets.SNOW_ACCOUNT }}
          SNOWSQL_USER:      ${{ secrets.SNOW_USER }}
          SNOWSQL_PWD:       ${{ secrets.SNOW_PASSWORD }}
          SNOWSQL_ROLE:      ${{ secrets.SNOW_ROLE }}
          SNOWSQL_WAREHOUSE: ${{ secrets.SNOW_WAREHOUSE }}
          SNOWSQL_DATABASE:  ${{ secrets.SNOW_DATABASE }}
          SNOWSQL_SCHEMA:    ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          cat > bootstrap.sql <<SQL
          USE ROLE ${SNOWSQL_ROLE};
          USE WAREHOUSE ${SNOWSQL_WAREHOUSE};
          USE DATABASE ${SNOWSQL_DATABASE};
          CREATE SCHEMA IF NOT EXISTS ${SNOWSQL_SCHEMA};
          USE SCHEMA ${SNOWSQL_SCHEMA};

          CREATE OR REPLACE TABLE DIM_BRAND (BRAND_ID INT, BRAND_CODE STRING);
          CREATE OR REPLACE TABLE DIM_CHANNEL (CHANNEL_ID INT, CHANNEL_NAME STRING);
          CREATE OR REPLACE TABLE DIM_CUSTOMER (CUSTOMER_ID INT, SIGNUP_UTC TIMESTAMP_NTZ, COUNTRY STRING);
          CREATE OR REPLACE TABLE FACT_SESSIONS (
            SESSION_ID INT, CUSTOMER_ID INT, SESSION_UTC TIMESTAMP_NTZ, BRAND STRING, CHANNEL STRING
          );
          CREATE OR REPLACE TABLE FACT_ORDERS (
            ORDER_ID INT, CUSTOMER_ID INT, ORDER_UTC TIMESTAMP_NTZ, BRAND STRING, CHANNEL STRING,
            UNITS NUMBER(9,0), REVENUE NUMBER(12,2), MARGIN NUMBER(12,2)
          );
          SQL
          "$HOME/.snowsql/snowsql" \
            -a "$SNOWSQL_ACCOUNT" -u "$SNOWSQL_USER" -r "$SNOWSQL_ROLE" \
            -w "$SNOWSQL_WAREHOUSE" -d "$SNOWSQL_DATABASE" -f bootstrap.sql

      # ───────────────────────────────────────────────────────────────────────────
      # Deploy repo SQL (now patched) — this will create your UDFs safely
      # ───────────────────────────────────────────────────────────────────────────
      - name: Deploy repo SQL (patched UDFs)
        if: steps.snowsql.outputs.ok == 'true'
        env:
          SNOWSQL_ACCOUNT:   ${{ secrets.SNOW_ACCOUNT }}
          SNOWSQL_USER:      ${{ secrets.SNOW_USER }}
          SNOWSQL_PWD:       ${{ secrets.SNOW_PASSWORD }}
          SNOWSQL_ROLE:      ${{ secrets.SNOW_ROLE }}
          SNOWSQL_WAREHOUSE: ${{ secrets.SNOW_WAREHOUSE }}
          SNOWSQL_DATABASE:  ${{ secrets.SNOW_DATABASE }}
          SNOWSQL_SCHEMA:    ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          # Run all tracked .sql files in a stable order (adjust globs/priorities if your repo needs it)
          mapfile -t FILES < <(git ls-files '*.sql' | sort)
          for f in "${FILES[@]}"; do
            echo ">>> Running $f"
            "$HOME/.snowsql/snowsql" \
              -a "$SNOWSQL_ACCOUNT" -u "$SNOWSQL_USER" -r "$SNOWSQL_ROLE" \
              -w "$SNOWSQL_WAREHOUSE" -d "$SNOWSQL_DATABASE" -f "$f"
          done

      # ───────────────────────────────────────────────────────────────────────────
      # (Optional) Demo data + analytics views using a Snowpark stored procedure
      # ───────────────────────────────────────────────────────────────────────────
      - name: Seed + Views (Snowpark SP + views)
        if: steps.snowsql.outputs.ok == 'true'
        env:
          SNOWSQL_ACCOUNT:   ${{ secrets.SNOW_ACCOUNT }}
          SNOWSQL_USER:      ${{ secrets.SNOW_USER }}
          SNOWSQL_PWD:       ${{ secrets.SNOW_PASSWORD }}
          SNOWSQL_ROLE:      ${{ secrets.SNOW_ROLE }}
          SNOWSQL_WAREHOUSE: ${{ secrets.SNOW_WAREHOUSE }}
          SNOWSQL_DATABASE:  ${{ secrets.SNOW_DATABASE }}
          SNOWSQL_SCHEMA:    ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          cat > seed_and_views.sql <<'SQL'
          USE ROLE ${SNOWSQL_ROLE};
          USE WAREHOUSE ${SNOWSQL_WAREHOUSE};
          USE DATABASE ${SNOWSQL_DATABASE};
          USE SCHEMA ${SNOWSQL_SCHEMA};

          -- Snowpark is supported in Python STORED PROCEDURES; ok to use pandas/numpy here.
          CREATE OR REPLACE PROCEDURE GENERATE_DEMO_DATA()
          RETURNS STRING
          LANGUAGE PYTHON
          RUNTIME_VERSION = '3.10'
          PACKAGES = ('snowflake-snowpark-python','pandas','numpy')
          HANDLER = 'run'
          AS
          $$
          from snowflake.snowpark import Session
          import pandas as pd, numpy as np

          def run(session: Session) -> str:
              rng = np.random.default_rng(42)
              brands   = ["JCREW","FACTORY","MADEWELL"]
              channels = ["Direct","Email","Paid Search","Social","Affiliate","Display"]

              session.create_dataframe(
                  pd.DataFrame({"BRAND_ID":[1,2,3],"BRAND_CODE":brands})
              ).write.mode("overwrite").save_as_table("DIM_BRAND")

              session.create_dataframe(
                  pd.DataFrame({"CHANNEL_ID":range(1,7),"CHANNEL_NAME":channels})
              ).write.mode("overwrite").save_as_table("DIM_CHANNEL")

              N = 50000
              start = pd.Timestamp("2024-09-01"); end = pd.Timestamp("2025-08-31")
              def random_times(n):
                  d = (end-start).total_seconds()
                  return [start + pd.Timedelta(seconds=float(rng.uniform(0, d))) for _ in range(n)]

              cust = pd.DataFrame({
                  "CUSTOMER_ID": range(1, N+1),
                  "SIGNUP_UTC":  random_times(N),
                  "COUNTRY":     rng.choice(["US","CA","UK"], size=N, p=[.84,.10,.06])
              })
              session.write_pandas(cust, "DIM_CUSTOMER", auto_create_table=True, overwrite=True)

              rows=[]; oid=1
              for cid in range(1,N+1):
                  k = int(np.clip(rng.poisson(1.4), 0, 6))
                  if k == 0: continue
                  times = sorted(random_times(k))
                  bsel  = rng.choice(brands,   size=k, p=[.55,.30,.15])
                  chsel = rng.choice(channels, size=k, p=[.45,.12,.18,.15,.06,.04])
                  for t, b, c in zip(times, bsel, chsel):
                      base  = 120 if b=="JCREW" else 85 if b=="MADEWELL" else 70
                      units = max(1, int(np.round(rng.gamma(2.0, 0.8))))
                      price = max(25, rng.normal(base, 18))
                      rev   = round(units * price, 2)
                      marg  = round(rev * rng.uniform(.48, .63), 2)
                      rows.append((oid, cid, str(t), b, c, units, rev, marg))
                      oid += 1

              df = pd.DataFrame(rows, columns=[
                "ORDER_ID","CUSTOMER_ID","ORDER_UTC","BRAND","CHANNEL","UNITS","REVENUE","MARGIN"
              ])
              session.write_pandas(df, "FACT_ORDERS", auto_create_table=True, overwrite=True)
              return f"Loaded {len(df):,} orders for {N:,} customers."
          $$;

          -- Example views (unchanged)
          CREATE OR REPLACE VIEW VW_FIRST_PURCHASE AS
          WITH f AS (
            SELECT CUSTOMER_ID, MIN(ORDER_UTC) AS FIRST_ORDER_UTC
            FROM FACT_ORDERS
            GROUP BY 1
          )
          SELECT o.CUSTOMER_ID, o.BRAND AS FIRST_PURCHASE_BRAND, f.FIRST_ORDER_UTC
          FROM FACT_ORDERS o
          JOIN f USING (CUSTOMER_ID)
          WHERE o.ORDER_UTC = f.FIRST_ORDER_UTC
          QUALIFY ROW_NUMBER() OVER (
            PARTITION BY o.CUSTOMER_ID ORDER BY o.ORDER_UTC, o.ORDER_ID
          ) = 1;

          CREATE OR REPLACE VIEW VW_BRAND_FLOWS_ANY AS
          SELECT fp.FIRST_PURCHASE_BRAND AS FROM_BRAND,
                 o.BRAND AS TO_BRAND,
                 COUNT(DISTINCT o.CUSTOMER_ID) AS CUSTOMERS
          FROM FACT_ORDERS o
          JOIN VW_FIRST_PURCHASE fp USING (CUSTOMER_ID)
          GROUP BY 1,2;

          CREATE OR REPLACE VIEW VW_BRAND_FLOWS_SUBSEQ AS
          WITH ft AS (
            SELECT CUSTOMER_ID, MIN(ORDER_UTC) AS FIRST_ORDER_UTC
            FROM FACT_ORDERS
            GROUP BY 1
          )
          SELECT fp.FIRST_PURCHASE_BRAND AS FROM_BRAND,
                 o.BRAND AS TO_BRAND,
                 COUNT(DISTINCT o.CUSTOMER_ID) AS CUSTOMERS
          FROM FACT_ORDERS o
          JOIN ft USING (CUSTOMER_ID)
          JOIN VW_FIRST_PURCHASE fp USING (CUSTOMER_ID)
          WHERE o.ORDER_UTC > ft.FIRST_ORDER_UTC
          GROUP BY 1,2;

          CALL GENERATE_DEMO_DATA();
          SQL

          sed -i "s/\${SNOWSQL_ROLE}/${SNOWSQL_ROLE}/g"         seed_and_views.sql
          sed -i "s/\${SNOWSQL_WAREHOUSE}/${SNOWSQL_WAREHOUSE}/g" seed_and_views.sql
          sed -i "s/\${SNOWSQL_DATABASE}/${SNOWSQL_DATABASE}/g"   seed_and_views.sql
          sed -i "s/\${SNOWSQL_SCHEMA}/${SNOWSQL_SCHEMA}/g"       seed_and_views.sql

          "$HOME/.snowsql/snowsql" \
            -a "$SNOWSQL_ACCOUNT" -u "$SNOWSQL_USER" -r "$SNOWSQL_ROLE" \
            -w "$SNOWSQL_WAREHOUSE" -d "$SNOWSQL_DATABASE" \
            -f seed_and_views.sql

      # Python fallback retained (connector) — unchanged from your file
      - name: Python fallback — install connector
        if: steps.snowsql.outputs.ok != 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Python fallback — run deploy script
        if: steps.snowsql.outputs.ok != 'true'
        env:
          SNOW_HOST:            ${{ secrets.SNOW_HOST }}
          SNOW_ACCOUNT:         ${{ secrets.SNOW_ACCOUNT }}
          SNOWFLAKE_ACCOUNT:    ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOW_USER:            ${{ secrets.SNOW_USER }}
          SNOW_PASSWORD:        ${{ secrets.SNOW_PASSWORD }}
          SNOW_ROLE:            ${{ secrets.SNOW_ROLE }}
          SNOW_WAREHOUSE:       ${{ secrets.SNOW_WAREHOUSE }}
          SNOW_DATABASE:        ${{ secrets.SNOW_DATABASE }}
          SNOW_SCHEMA:          ${{ secrets.SNOW_SCHEMA }}
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip
          python -m pip install snowflake-connector-python pandas numpy
          python - <<'PY'
          import os, snowflake.connector as sf
          host=(os.getenv('SNOW_HOST') or '').strip()
          acct=(os.getenv('SNOW_ACCOUNT') or os.getenv('SNOWFLAKE_ACCOUNT') or '').strip()
          user=os.environ['SNOW_USER']; pwd=os.environ['SNOW_PASSWORD']
          role=os.environ['SNOW_ROLE']; wh=os.environ['SNOW_WAREHOUSE']
          db=os.environ['SNOW_DATABASE']; sc=os.environ['SNOW_SCHEMA']
          kwargs=dict(user=user,password=pwd,role=role,warehouse=wh,database=db,schema=sc)
          if host: kwargs['host']=host
          if acct: kwargs['account']=acct
          if ('host' not in kwargs) and ('account' not in kwargs):
              raise RuntimeError("Provide SNOW_ACCOUNT or SNOW_HOST.")
          ctx=sf.connect(**kwargs); cs=ctx.cursor()
          try:
              def exec(sql): print("SQL>", sql.splitlines()[0][:100], "..."); cs.execute(sql)
              exec(f"USE ROLE {role}")
              exec(f"USE WAREHOUSE {wh}")
              exec(f"USE DATABASE {db}")
              exec(f"CREATE SCHEMA IF NOT EXISTS {sc}")
              exec(f"USE SCHEMA {sc}")
              # create base tables (same as SnowSQL path) ...
              exec("CREATE OR REPLACE TABLE DIM_BRAND (BRAND_ID INT, BRAND_CODE STRING)")
              exec("CREATE OR REPLACE TABLE DIM_CHANNEL (CHANNEL_ID INT, CHANNEL_NAME STRING)")
              exec("CREATE OR REPLACE TABLE DIM_CUSTOMER (CUSTOMER_ID INT, SIGNUP_UTC TIMESTAMP_NTZ, COUNTRY STRING)")
              exec("CREATE OR REPLACE TABLE FACT_SESSIONS (SESSION_ID INT, CUSTOMER_ID INT, SESSION_UTC TIMESTAMP_NTZ, BRAND STRING, CHANNEL STRING)")
              exec("CREATE OR REPLACE TABLE FACT_ORDERS (ORDER_ID INT, CUSTOMER_ID INT, ORDER_UTC TIMESTAMP_NTZ, BRAND STRING, CHANNEL STRING, UNITS NUMBER(9,0), REVENUE NUMBER(12,2), MARGIN NUMBER(12,2))")
              # (Keep the rest of your fallback logic here if needed)
          finally:
              cs.close(); ctx.close()
          PY
